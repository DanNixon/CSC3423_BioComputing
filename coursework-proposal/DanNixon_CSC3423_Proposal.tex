\documentclass[a4paper]{article}

% Set for specific document
\def\DOCTITLE{CSC3423 Biocomputing Coursework Proposal}
\def\DOCAUTHOR{Dan Nixon (120263697)}
\def\DOCDATE{05/11/2015}

% Set document attributes
\title{\DOCTITLE}
\author{\DOCAUTHOR}
\date{\DOCDATE}

\usepackage{fullpage}
\usepackage{scrextend}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[style=numeric-comp,natbib=true]{biblatex}
\usepackage{minted}

\addbibresource{DanNixon_CSC3423_Proposal.bib}

% Handle graphics correctly
\ifx\pdftexversion\undefined
\usepackage{graphicx}
% \usepackage[dvips]{graphicx}
\else
\usepackage[pdftex]{graphicx}
\DeclareGraphicsRule{*}{mps}{*}{}
\fi

% Setup headers and footers
\pagestyle{fancy}
\lhead{}
\chead{\DOCTITLE}
\rhead{}
\rfoot{\DOCDATE}
\cfoot{\thepage}
\lfoot{\DOCAUTHOR}

% New page for each section
\newcommand{\sectionbreak}{\clearpage}

% Set header and footer sizes
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\setlength{\headheight}{15.2pt}
\setlength{\headsep}{15.2pt}

% Number equations per block (rather than per line)
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\begin{document}

\maketitle

\begin{abstract}
  This document describes the choices I have made for the two nature inspired
  algorithms, their parameters and knowledge representation that will be used to
  solve the given classification problem.
\end{abstract}

\section{Genetic algorithm}

The first algorithm I have chosen is a genetic algorithm using a hyperrectangle
classifier.

The advantages of the genetic algorithm is its ability to work with a wide range
of optimisation and machine learning problems this is inherently because the
algorithm itself does not have knowledge of the problem it is trying to solve,
instead it operates by making small random changes to a population of candidate
solutions.

This property also allows the algorithm to remain unchanged as the
classification problem it is solving increases to higher dimensions.

\subsection{Knowledge Representation}

The knowledge representation I will use is a hyperrectangle. For a
classification problem in $n$ dimensions a classifier will be stored as an
origin position in $n$ dimensions and a length in each dimension as shown for a
three dimensional classifier in figure \ref{fig:alg1_kr}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{out/alg1_kr.eps}
  \caption{3D hyperrectangle classifier}
  \label{fig:alg1_kr}
\end{figure}

The advantage of the hyperrectangle classifier is that it is a very small data
set, in a classification problem its is a set of upper and lower bounds for each
dimension of the problem.

The simplicity of this knowledge representation also significantly simplifies
and increases the speed of computation in other stages of the genetic algorithm,
namely selection and crossover.

During selection the fitness function for each chromosome must be calculated, as
with a hyperrectangle classifier this involves simply determining if a given
instance is within all bounds of the classifier this operation is very
computationally inexpensive.

During crossover two new children can derived using the blend alpha
(BLX-$\mathrm{\alpha}$) function described by Takahashi and Kita \cite{blxa},
which is again an inexpensive operation due to the simplicity of the knowledge
representation.

\subsection{Implementation}

When a new classifier is created it will take a random size within the bounding
box of the instances in the current training set, the size will be determined
independently for each axis and will be selected using a linear distribution
from values between a minimum percentage of the axis length (which is a
parameter of the system) and the axis length.

Equation \ref{eq:blxa} shows the implementation that will be used for the
BLX-$\mathrm{\alpha}$ crossover function to determine a parameter of classifier
$x^{c}$ (a child of classifiers $x^{a}$ and $x^{b}$) \cite{blxa}.

\begin{align*}
  x^{c}_{i} &\in [X^{a}_{i}, X^{b}_{i}] \\
  X^{a}_{i} &= min(x^{a}_{i}, x^{a}_{i}) - \alpha d_{i} \\
  X^{b}_{i} &= max(x^{b}_{i}, x^{b}_{i}) + \alpha d_{i} \\
  d_{i} &= |x^{1}_{i} - x^{2}_{i}|
  \numberthis
  \label{eq:blxa}
\end{align*}

Due to the speed of operations over a single candidate classifier, a relatively
large population size will be selected, initially around 100 candidates however
this parameter will be tuned to optimise classification correctness and
evolution speed.

The best candidates will be selected through tournament selection where the
tournament size will initially be set to two candidates, however I will attempt
to set this parameter as a function of the iteration count or overall change in
fitness of the population in order to increase the speed of evolution.

A brief overview of the planned technical implementation of the genetic
algorithm is described in figure \ref{fig:alg1_uml}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{out/alg1_uml.1}
  \caption{Genetic algorithm implementation UML diagram}
  \label{fig:alg1_uml}
\end{figure}

The genetic algorithm will be implemented using the JGAP \cite{jgap} Java
library.

A \textit{Chromesome} object will possess $2n$ \textit{DoubleGene} objects where
$n$ is the dimensionality of the classification problem, the first $n$ genes
will store the origin point in $n$ dimension space and the remaining $n$ genes
will store the bounds of the hyperrectngle in each of its dimensions.

The value of each gene will be limited to be within the hyperrectangle that
bounds the points remaining in the training set on a new call to
\texttt{generateSubsolution()}.

\section{Neural network}

The first algorithm I have chosen is a multi layer neural network with all
perceptrons using the sigmoid activation function.

The continuous logarithmic sigmoid function (described by equation
\ref{eq:sigmoid}) will be used as a non linear activation function is required
for classification of the given dataset, which contains shapes primarily made of
circles and curves.

This is also one of the simplest functions to calculate the derivative of when
recalculating the weights of inputs to each perceptron during back propagation.

\begin{equation}
  \sigma(t, \beta) = \frac{1}{1 + e^{- \beta t}}
  \label{eq:sigmoid}
\end{equation}

The structure of the neural network will see $n$ perceptrons in the first
(input) layer for an $n$ dimensional classification problem, this is a given by
the fact that the size of the input layer must match the dimensionality of the
problem.

Similarly, for a classification problem, the output layer will consist of a
single perceptron.

The neural network will initially only contain a single inner layer as the
majority of classifications problems can usually be solved using a single inner
layer \cite{nninjava}.

The number of perceptrons in the inner layer will initially be 5 but will be
tuned to obtain best performance from the neural network in terms of learning
speed and classification accuracy.

\subsection{Implementation}

The planned technical implementation of the neural network is described in
figure \ref{fig:alg2_uml}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{out/alg2_uml.1}
  \caption{Neural network implementation UML diagram}
  \label{fig:alg2_uml}
\end{figure}

The neural network will be implemented using the Neuroph \cite{neuroph} Java
library, specifically the \textit{MultiLayerPerceptron} class.

When a new call is made to \textit{generateSubsolution()} a new \textit{DataSet}
will be created of all the instances remaining in the training set, this will
form the training set for the neural network.

Listing \ref{list:ClassifierNN} describes the way in which the neural network
will be crated and trained in the \texttt{ClassifierNN} class using the JGAP
API.

\begin{listing}[H]
  \begin{minted}[frame=lines]{text}

create two dimensional DataSet
for each Instance in training set
  create new DataSetRow and add to DataSet
create new MultiLayerPerceptron
train MultiLayerPerceptron with training set

  \end{minted}
  \caption{\texttt{ClassifierNN} constructor pseudo code}
  \label{list:ClassifierNN}
\end{listing}

Listing \ref{list:ClassifierNN_classifyInstance} describes, how after being
trained, the neural network will be used to classify a given instance.

\begin{listing}[H]
  \begin{minted}[frame=lines]{text}

create new DataSetRow from Instance data
set DataSetRow as input to neural network
calculate neural network output
return value of output perceptron

  \end{minted}
  \caption{\texttt{ClassifierNN.classifyInstance()} pseudo code}
  \label{list:ClassifierNN_classifyInstance}
\end{listing}

The knowledge representation of the neural network is composed of the weights
assigned to each perceptron input, the JGAP library also allows this information
to be saved to a file if required.

\printbibliography

\end{document}
