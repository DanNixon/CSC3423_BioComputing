\documentclass[a4paper]{article}

% Set for specific document
\def\DOCTITLE{CSC3423 Biocomputing Coursework Report}
\def\DOCAUTHOR{Dan Nixon (120263697)}
\def\DOCDATE{05/11/2015}

% Set document attributes
\title{\DOCTITLE}
\author{\DOCAUTHOR}
\date{\DOCDATE}

\usepackage{fullpage}
\usepackage{scrextend}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{booktabs}

% Handle graphics correctly
\ifx\pdftexversion\undefined
\usepackage{graphicx}
% \usepackage[dvips]{graphicx}
\else
\usepackage[pdftex]{graphicx}
\DeclareGraphicsRule{*}{mps}{*}{}
\fi

% Setup headers and footers
\pagestyle{fancy}
\lhead{}
\chead{\DOCTITLE}
\rhead{}
\rfoot{\DOCDATE}
\cfoot{\thepage}
\lfoot{\DOCAUTHOR}

% New page for each section
\newcommand{\sectionbreak}{\clearpage}

% Set header and footer sizes
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\setlength{\headheight}{15.2pt}
\setlength{\headsep}{15.2pt}

\begin{document}

\maketitle

\begin{abstract}
  This report will give an overview of the two nature inspired algorithms that
  were implemented to solve the classification problem, how they differ from
  what was outlined in the proposal and a critical evaluation between the
  performance of both in terms of learning time and classification accuracy.
\end{abstract}

\section{Genetic Algorithm}
\label{sec:ga}

TODO

\subsection{Implementation}
\label{sec:ga_implementation}

The implementation of the genetic algorithm mostly follows what was described in
the proposal document however with several important changes that were required
as part of an optimisation or technical limitation.

The first notable change is the data format of the knowledge representation,
previously this would have stored an "origin" point and a length in each
dimension to denote the bounds of the hyperrectangle.

In the final implementation this has been changed to storing two points
diagonally opposite from each other as shown in figure \ref{fig:ga_kr}, this was
due to an optimisation that allowed the values of the genes that represented the
coordinates of each point to be constrained between two values, this was done to
limit the search space the mutations of a new classifier would operate within in
order to converge to the best solution faster.

This optimisation simply performs a linear search through each \texttt{Instance}
of the training \texttt{InstanceSet} in order to obtain the minimum and maximum
values for each dimension and then sets the limits of the relevant
\texttt{DoubleGene} to these values plus a padding constant.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{out/alg1_kr.eps}
  \caption{3D hyperrectangle classifier}
  \label{fig:ga_kr}
\end{figure}

It was also my intention to add validation to the \texttt{CompositeGene} which
would hold the upper and lower positions of a single dimension, however instead
of generating a new mutation on a failed validation the JGAP framework instead
aborted the entire evolution step.

The implementation of the genetic algorithm was done using the JGAP \cite{jgap}
Java library, the structure of the code the was used to integrate this into the
provided framework is shown in figure \ref{fig:ga_uml}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{out/alg1_uml.1}
  \caption{Genetic algorithm implementation UML diagram}
  \label{fig:ga_uml}
\end{figure}

The final implementation uses a selection algorithm that selects the top
candidates based on their fitness value, 

\subsection{Tuning}
\label{sec:ga_tuning}

TODO

\begin{table}[h!]
  \centering
  \begin{tabular}{@{}lllllll@{}}
    \toprule
    Population Size & Learn Rate & Selector       & Accuracy (\%) & Error (\%) & Unclassified (\%) & Time (s) \\
    \midrule
    100             & 0.9        & Best (0.9)     & 95.26         & 4.74       & 0                 & 23.96    \\
    100             & 0.95       & Best (0.9)     & 97.37         & 2.63       & 0                 & 33.622   \\
    100             & 0.99       & Best (0.9)     & 95.79         & 4.21       & 0                 & 46.085   \\
    20              & 0.9        & Best (0.9)     & 94.21         & 5.79       & 0                 & 14.269   \\
    20              & 0.95       & Best (0.9)     & 94.74         & 5.26       & 0                 & 19.23    \\
    20              & 0.99       & Best (0.9)     & 95.79         & 4.21       & 0                 & 23.143   \\
    200             & 0.9        & Best (0.9)     & 95.26         & 4.21       & 0.53              & 56.417   \\
    200             & 0.95       & Best (0.9)     & 94.21         & 5.79       & 0                 & 60.465   \\
    200             & 0.99       & Best (0.9)     & 94.74         & 5.26       & 0                 & 73.05    \\
    100             & 0.9        & Tournament (2) & 95.79         & 4.21       & 0                 & 53.039   \\
    100             & 0.95       & Tournament (2) & 95.79         & 4.21       & 0                 & 67.503   \\
    100             & 0.99       & Tournament (2) & 96.84         & 3.16       & 0                 & 93.144   \\
    20              & 0.9        & Tournament (2) & 94.21         & 5.26       & 0.53              & 35.135   \\
    20              & 0.95       & Tournament (2) & 96.32         & 3.68       & 0                 & 50.156   \\
    20              & 0.99       & Tournament (2) & 91.58         & 8.42       & 0                 & 32.734   \\
    200             & 0.9        & Tournament (2) & 94.74         & 5.26       & 0                 & 77.3     \\
    200             & 0.95       & Tournament (2) & 96.32         & 3.68       & 0                 & 83.885   \\
    200             & 0.99       & Tournament (2) & 95.79         & 4.21       & 0                 & 100.731  \\
    100             & 0.9        & Tournament (5) & 95.26         & 4.74       & 0                 & 66.057   \\
    100             & 0.95       & Tournament (5) & 96.32         & 3.68       & 0                 & 82.002   \\
    100             & 0.99       & Tournament (5) & 95.79         & 3.68       & 0.53              & 106.346  \\
    20              & 0.9        & Tournament (5) & 93.16         & 6.84       & 0                 & 37.7     \\
    20              & 0.95       & Tournament (5) & 94.21         & 5.79       & 0                 & 49.235   \\
    20              & 0.99       & Tournament (5) & 89.47         & 10.53      & 0                 & 41.093   \\
    200             & 0.9        & Tournament (5) & 95.79         & 4.21       & 0                 & 138.576  \\
    200             & 0.95       & Tournament (5) & 94.21         & 5.79       & 0                 & 103.349  \\
    200             & 0.99       & Tournament (5) & 95.79         & 4.21       & 0                 & 129.103  \\
    \bottomrule
  \end{tabular}
  \caption{Genetic algorithm tuning}
  \label{tab:ga_tuning}
\end{table}

TODO

\section{Neural Network}
\label{sec:nn}

TODO

\subsection{Implementation}
\label{sec:nn_implementation}

TODO

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{out/alg2_uml.1}
  \caption{Neural network implementation UML diagram}
  \label{fig:nn_uml}
\end{figure}

TODO

\subsection{Tuning}
\label{sec:nn_tuning}

TODO

\begin{table}[h!]
  \centering
  \begin{tabular}{@{}lllllll@{}}
    \toprule
    Transfer Function  & Topology     & Learning Rate & Accuracy (\%) & Error (\%) & Unclassified (\%) & Time (s)   \\
    \midrule
    Sigmoid            & 2, 5, 1      & 0.01          & 91.05         & 8.95       & 0                 & 14.075     \\
    Hyperbolic Tangent & 2, 5, 1      & 0.01          & 92.63         & 7.37       & 0                 & 14.396     \\
    Sigmoid            & 2, 10, 1     & 0.01          & 83.68         & 16.32      & 0                 & 22.149     \\
    Hyperbolic Tangent & 2, 10, 1     & 0.01          & 94.74         & 5.26       & 0                 & 25.602     \\
    Sigmoid            & 2, 20, 20, 1 & 0.01          & 97.37         & 2.63       & 0                 & 11.453     \\
    Hyperbolic Tangent & 2, 20, 20, 1 & 0.01          & 97.37         & 2.63       & 0                 & 7.098      \\
    Sigmoid            & 2, 20, 10, 1 & 0.01          & 97.37         & 2.63       & 0                 & 6.355      \\
    Hyperbolic Tangent & 2, 20, 10, 1 & 0.01          & 97.37         & 2.63       & 0                 & 2.667      \\
    Sigmoid            & 2, 10, 10, 1 & 0.01          & 92.11 (1)     & 7.89 (1)   & 0                 & 63.059 (1) \\
    Hyperbolic Tangent & 2, 10, 10, 1 & 0.01          & 97.37         & 2.63       & 0                 & 3.728      \\
    \bottomrule
  \end{tabular}
  \caption{Neural network tuning}
  \label{tab:nn_tuning}
\end{table}

TODO

\section{Comparison of algorithms}
\label{sec:comparison}

A comparison between the performance of the two algorithms, both in terms of the
time taken for learning and the accuracy of the resulting classification was
performed using a dataset of 100 runs of each algorithm using the optimal
parameters found in sections \ref{sec:ga_tuning} and \ref{sec:nn_tuning}.

Statistics of the results of this performance test are shown in table
\ref{tab:comparison_tab}.

\begin{table}[h!]
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    Matric                        & Genetic Algorithm & Neural Network \\
    \midrule
    Execution Time (s, avg.)      & 37.252            & 5.086          \\
    Execution Time (s, std. dev.) & 5.122             & 1.695          \\
    Execution Time (s, min.)      & 30.749            & 2.418          \\
    Execution Time (s, max.)      & 45.103            & 8.398          \\
    Accuracy (\%, avg.)           & 96.4              & 97.21          \\
    Accuracy (\%, std. dev.)      & 1.12              & 0.96           \\
    Accuracy (\%, min.)           & 94.74             & 95.26          \\
    Accuracy (\%, max.)           & 97.89             & 98.42          \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of algorithm performance (100 executions)}
  \label{tab:comparison_avg}
\end{table}

TODO

% \begin{thebibliography}{9}
% \end{thebibliography}

\end{document}
